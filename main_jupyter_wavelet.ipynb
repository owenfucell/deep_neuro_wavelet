{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pywt\n",
    "\n",
    "import lib.matnpy.matnpyio as io\n",
    "import lib.matnpy.matnpy as matnpy\n",
    "import lib.cnn.helpers as hlp\n",
    "import lib.cnn.cnn as cnn\n",
    "#from lib.cnn_1.confusion_matrix import plot_confusion_matrix\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib for the jupyter only\n",
    "import sklearn.metrics # classification_report , confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erfinv\n",
    "from scipy.special import erf\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the jupyter\n",
    "\n",
    "def rapport(confusion_matrix):\n",
    "    \"\"\"\n",
    "    This function returns performance measures for the classification tasks \n",
    "    \"\"\"\n",
    "    dico = {}\n",
    "    classes = len(confusion_matrix)\n",
    "    \n",
    "    if classes == 2 :\n",
    "        \n",
    "        tp = cm[0][0]\n",
    "        fp = cm[0][1]\n",
    "        fn = cm[1][0]\n",
    "        tn = cm[1][1]\n",
    "        \n",
    "        dico['accuracy'] = tp + tn /(tp +fn +fp +tn)\n",
    "        dico['precision'] = tp/(tp +fp)\n",
    "        dico['recall'] = tp/(tp + fn) # Sensitivity\n",
    "        beta = 1\n",
    "        dico['Fscore1'] = (beta**2 +1)*tp/( (beta**2 +1)*tp + beta**2*fn +fp )\n",
    "        dico['Specificity'] = tn/ (fp + tn)\n",
    "        dico['AUC'] = 0.5 *( tp /(tp +fn) + tn/(tn + fp) )\n",
    "    else:\n",
    "        fp = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "        fn = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "        tp = np.diag(confusion_matrix)\n",
    "        tn = confusion_matrix.sum() - (fp + fn + tp)\n",
    "\n",
    "        dico['average_accuracy'] = np.mean( (tp + tn)/(tp + fn + fp + tn))\n",
    "        dico['error_rate'] = np.mean( (fp + fn)/(tp +fn + fp +tn) )\n",
    "\n",
    "        #micro\n",
    "\n",
    "\n",
    "        dico['precision_micro'] = np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "        dico['recall_micro'] = np.sum(tp) / (np.sum(tp) +np.sum(fn))\n",
    "        beta = 1\n",
    "        dico['Fscore1_micro'] = (beta**2+1)*dico['precision_micro'] * dico['recall_micro']/( beta **2 * dico['precision_micro']+dico['recall_micro'])\n",
    "\n",
    "        # macro\n",
    "\n",
    "        dico['precision_macro'] = np.mean(  tp/(tp+fp) )\n",
    "        dico['recall_macro'] = np.mean(  tp/(tp+fn) )\n",
    "        beta = 1\n",
    "        dico['Fscore1_macro'] = (beta**2+1)*dico['precision_macro'] * dico['recall_macro']/( beta **2 * dico['precision_macro']+dico['recall_macro'])\n",
    "    return(dico)\n",
    "\n",
    "#from scipy.special import erfinv\n",
    "#from scipy.special import erf\n",
    "\n",
    "def get_recall_macro(confusion_matrix,  alpha = erf(1/np.sqrt(2)) ):\n",
    "    \"\"\"\n",
    "    This function returns the recall macro*, its error bar for a confidence interval of alpha (default at 68%),\n",
    "    the accuracy per class, and the number of element of each class.\n",
    "    \n",
    "    recall macro is the mean of accuracy per class.\n",
    "    \"\"\"\n",
    "    n_test = np.sum(confusion_matrix, axis = 1)\n",
    "    \n",
    "    classes = confusion_matrix.shape[0]\n",
    "    \n",
    "    \n",
    "    #fp = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "    fn = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    tp = np.diag(confusion_matrix)\n",
    "    #tn = confusion_matrix.sum() - (fp + fn + tp)\n",
    "    \n",
    "    #\n",
    "    recall_macro_per_class = tp/(tp+fn)\n",
    "    #print(recall_macro_per_class)\n",
    "    #\n",
    "    recall_macro = np.mean(recall_macro_per_class)\n",
    "    \n",
    "    # l'écart-type théorique du recall macro, soit un interval de confiance de 0.68 = erf( 1/np.sqrt(2)) --> 1 fois l'écart-type\n",
    "    error_bar = np.sqrt(2) * erfinv(alpha) *np.sqrt( np.sum(recall_macro_per_class * (1 - recall_macro_per_class)/n_test) ) /classes \n",
    "    \n",
    "    return(recall_macro, error_bar, recall_macro_per_class, n_test )\n",
    "\n",
    "#import itertools\n",
    "def plot_confusion_matrix(cm, classes_names,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes_names))\n",
    "    plt.xticks(tick_marks, classes_names, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_names)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_decomposition(data, depth_wav , w_name = 'db4', mode ='smooth'):\n",
    "    w = pywt.Wavelet(w_name)\n",
    "    a = data.copy()\n",
    "    ca = []\n",
    "    cd = []\n",
    "    for i in range(depth_wav):\n",
    "        (a, d) = pywt.dwt(a, w, mode)\n",
    "        a = a[:,:,1:-2]\n",
    "        d = d[:,:,1:-2]\n",
    "        ca.append(a)\n",
    "        cd.append(d)\n",
    "    return(ca, cd)\n",
    "\n",
    "def init_weights(shape, dist='random_normal', normalized=True):\n",
    "    \"\"\"Initializes network weights.\n",
    "    \n",
    "    Args:\n",
    "        shape: A tensor. Shape of the weights.\n",
    "        dist: A str. Distribution at initialization, one of 'random_normal' or \n",
    "            'truncated_normal'.\n",
    "        normalized: A boolean. Whether weights should be normalized.\n",
    "        \n",
    "    Returns:\n",
    "        A tf.variable.\n",
    "    \"\"\"\n",
    "    # Normalized if normalized set to True\n",
    "    if normalized == True:\n",
    "        denom = np.prod(shape[:-1])\n",
    "        std = 1e-3 / denom\n",
    "    else:\n",
    "        std = 1e-4\n",
    "    \n",
    "    # Draw from random or truncated normal\n",
    "    if dist == 'random_normal':\n",
    "        weights = tf.random_normal(shape, stddev=std)\n",
    "    elif dist == 'truncated_normal':\n",
    "        weights = tf.truncated_normal(shape, stddev=0.1)\n",
    "    \n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    \"\"\"2D convolution. \"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, pool_dim):\n",
    "    \"\"\"Max pooling. \"\"\"\n",
    "    patch_height = pool_dim[0]\n",
    "    patch_width = pool_dim[1]\n",
    "    return tf.nn.max_pool(x, \n",
    "                          ksize=[1, patch_height, patch_width, 1], \n",
    "                          strides=[1, patch_height, patch_width, 1], \n",
    "                          padding='SAME')\n",
    "\n",
    "def l2_loss(weights, l2_regularization_penalty, y_, y_conv, name):\n",
    "    \"\"\"Implements L2 loss for an arbitrary number of weights.\n",
    "    \n",
    "    Args:\n",
    "        weights: A dict. One key/value pair per layer in the network.\n",
    "        l2_regularization_penalty: An int. Scales the l2 loss arbitrarily.\n",
    "        y_:\n",
    "        y_conv:\n",
    "        name: \n",
    "            \n",
    "    Returns:\n",
    "        L2 loss.        \n",
    "    \"\"\"\n",
    "    w = {}\n",
    "    for key, value in weights.items():\n",
    "        w[key] = tf.nn.l2_loss(value)\n",
    "    \n",
    "    l2_loss = l2_regularization_penalty * sum(w.values())\n",
    "    \n",
    "    unregularized_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    return tf.add(unregularized_loss, l2_loss, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# PARAMS #\n",
    "##########\n",
    "sess = '01' # '01' or '02'\n",
    "sess_no = '150210'\n",
    "decode_for = 'stim'\n",
    "\n",
    "#data path\n",
    "raw_path = ('/media/rudy/disk2/lucy/' + sess_no +'/session01/')\n",
    "#raw_path = ('/media/rudy/disk2/lucy/'+ sess_no +'/session01/')\n",
    "rinfo_path = raw_path + 'recording_info.mat'\n",
    "tinfo_path = raw_path + 'trial_info.mat'\n",
    "\n",
    "#target_area = ['V1']\n",
    "target_cortex = 'Visual'\n",
    "target_area = io.get_area_cortex(rinfo_path, target_cortex, unique = True)\n",
    "\n",
    "align_on ,from_time, to_time ='sample', -6, 506\n",
    "\n",
    "\n",
    "mode = 'smooth'\n",
    "elec_type = 'grid'  # any one of single|grid|average\n",
    "\n",
    "#only_correct_trials = False  \n",
    "if decode_for == 'stim':\n",
    "    only_correct_trials = True\n",
    "else:\n",
    "    only_correct_trials = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN PARAMS\n",
    "\n",
    "n_layers = 6\n",
    "depth_wav = 7\n",
    "in_1, out_1 = 1, 6\n",
    "in_2, out_2 = 7, 21\n",
    "in_3, out_3 = 22, 41\n",
    "in_4, out_4 = 42, 69\n",
    "in_5, out_5 = 70,97 \n",
    "in_6, out_6 = 98, 122\n",
    "fc_units = 100\n",
    "\n",
    "\n",
    "pool_dim = [1, 2] #size of max_pool\n",
    "patch_dim = [1, 7] # taille de la fenêtre de convolution\n",
    "patch_dim5 = [1, 3]\n",
    "patch_dim6 = [1, 2]\n",
    "nonlin = 'elu' # leaky_relu'\n",
    "\n",
    "n_iterations = 2000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "size_of_batches = 15\n",
    "\n",
    "keep_prob_train = .2\n",
    "l2_regularization_penalty = 0#0.001\n",
    "amplify_input = True # q**..\n",
    "q = 4\n",
    "\n",
    "dist = 'random_normal'\n",
    "normalized_weights = True\n",
    "\n",
    "bn = False # Indicating whether batch-norm shoud be applied\n",
    "batch_norm = 'renorm'  # 'after'\n",
    "DECAY = .999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "# train/test size, random split \n",
    "# train_size = .8\n",
    "# test_size = .2\n",
    "seed = 8392 # np.random.randint(1,10000)\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "# Auto-define number of classes\n",
    "classes = 2 if decode_for == 'resp' else 5\n",
    "\n",
    "# Load data and targets\n",
    "data = matnpy.get_subset_by_areas(sess_no, raw_path, \n",
    "                         align_on, from_time, to_time, \n",
    "                         target_area,\n",
    "                         only_correct_trials = only_correct_trials, renorm = False, elec_type = elec_type )\n",
    "n_chans = data.shape[1]\n",
    "\n",
    "targets = io.get_targets(decode_for, raw_path,n_chans, elec_type=elec_type,\n",
    "                        only_correct_trials=only_correct_trials,\n",
    "                        onehot=True)\n",
    "\n",
    "# indices = np.arange(len(data))\n",
    "# train, test, train_labels, test_labels, idx_train, idx_test = (\n",
    "#         train_test_split(\n",
    "#             data, \n",
    "#             targets, \n",
    "#             indices,\n",
    "#             test_size=test_size, \n",
    "#             random_state=seed)\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelet decomposition        \n",
    "depth_wav = 7\n",
    "ca, cd = wavelet_decomposition(data, depth_wav=depth_wav) # ca size = depth_wav * n_trials * n_chans * 2*8-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESHAPE DATA\n",
    "\n",
    "depth_wav = 7\n",
    "ca_train = 7 *[0]\n",
    "cd_train = 7 *[0]\n",
    "for i in range(depth_wav):\n",
    "    ca_train[i] = np.reshape(np.array(ca[i]),(len(ca[i]), len(ca[i][0]), len(ca[i][0][0]), 1))\n",
    "    cd_train[i] = np.reshape(np.array(cd[i]),(len(cd[i]), len(cd[i][0]), len(cd[i][0][0]), 1))\n",
    "\n",
    "\n",
    "ca = ca_train\n",
    "cd = cd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AMPLIFY INPUT    \n",
    "if amplify_input == True :\n",
    "    for i in range(len(ca)):\n",
    "        cd[i] = 10000 * (q**(len(ca)-i-1)) *cd[i]**2\n",
    "\n",
    "## slit train/ test\n",
    "#ca_train = [ca[i][idx_train, :, : , :] for i in range(len(ca))]\n",
    "#cd_train = [cd[i][idx_train, :, : , :] for i in range(len(cd))]\n",
    "\n",
    "#ca_test = [ca[i][idx_test, :, : , :] for i in range(len(ca))]\n",
    "#cd_test = [cd[i][idx_test, :, : , :] for i in range(len(cd))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## \n",
    "# CREATE CNN #\n",
    "##############\n",
    "\n",
    "if elec_type == 'single':\n",
    "    n_chans = 1\n",
    "x_1 = tf.placeholder(tf.float32, shape=[None, n_chans , 128, 1])\n",
    "x_2 = tf.placeholder(tf.float32, shape=[None, n_chans , 64, 1])\n",
    "x_3 = tf.placeholder(tf.float32, shape=[None, n_chans , 32, 1])\n",
    "x_4 = tf.placeholder(tf.float32, shape=[None, n_chans , 16, 1])\n",
    "x_5 = tf.placeholder(tf.float32, shape=[None, n_chans , 8, 1])\n",
    "x_6 = tf.placeholder(tf.float32, shape=[None, n_chans , 4, 1])\n",
    "\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, classes])\n",
    "\n",
    "training = tf.placeholder_with_default(True, shape=())\n",
    "\n",
    "weights = {}\n",
    "\n",
    "# first_layer\n",
    "weights_1 = init_weights([patch_dim[0], patch_dim[1], in_1, out_1], \n",
    "                    dist=dist,\n",
    "                    normalized=normalized_weights)\n",
    "conv_1 = conv2d(x_1, weights_1)\n",
    "conv_1_elu = tf.nn.elu(conv_1)\n",
    "if bn == True:\n",
    "    conv_1_bn_elu = tf.contrib.layers.batch_norm(\n",
    "        conv_1_elu,\n",
    "        data_format='NHWC',\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        is_training=training,\n",
    "        decay=DECAY,\n",
    "        renorm=True)\n",
    "    maxpool_1_bn_elu = max_pool(conv_1_bn_elu, pool_dim)\n",
    "else:\n",
    "    maxpool_1_bn_elu = max_pool(conv_1_elu, pool_dim)\n",
    "\n",
    "weights[0] = weights_1    \n",
    "\n",
    "# second layer\n",
    "concat_2 = tf.concat((maxpool_1_bn_elu, x_2), axis = 3)\n",
    "weights_2 = init_weights([patch_dim[0], patch_dim[1], in_2, out_2], \n",
    "                    dist=dist,\n",
    "                    normalized=normalized_weights)\n",
    "conv_2 = conv2d(concat_2, weights_2)\n",
    "conv_2_elu = tf.nn.elu(conv_2)\n",
    "if bn == True:\n",
    "    conv_2_bn_elu = tf.contrib.layers.batch_norm(\n",
    "        conv_2_elu,\n",
    "        data_format='NHWC',\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        is_training=training,\n",
    "        decay=DECAY,\n",
    "        renorm=True)\n",
    "    maxpool_2_bn_elu = max_pool(conv_2_bn_elu, pool_dim)\n",
    "else:\n",
    "    maxpool_2_bn_elu = max_pool(conv_2_elu, pool_dim)\n",
    "\n",
    "weights[1] = weights_2  \n",
    "\n",
    "# 3 layer\n",
    "concat_3 = tf.concat((maxpool_2_bn_elu, x_3), axis = 3)\n",
    "weights_3 = init_weights([patch_dim[0], patch_dim[1], in_3, out_3], \n",
    "                    dist=dist,\n",
    "                    normalized=normalized_weights)\n",
    "conv_3 = conv2d(concat_3, weights_3)\n",
    "conv_3_elu = tf.nn.elu(conv_3)\n",
    "if bn == True:\n",
    "    conv_3_bn_elu = tf.contrib.layers.batch_norm(\n",
    "        conv_3_elu,\n",
    "        data_format='NHWC',\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        is_training=training,\n",
    "        decay=DECAY,\n",
    "        renorm=True)\n",
    "    maxpool_3_bn_elu = max_pool(conv_3_bn_elu, pool_dim)\n",
    "else:\n",
    "    maxpool_3_bn_elu = max_pool(conv_3_elu, pool_dim)\n",
    "\n",
    "weights[2] = weights_3  \n",
    "\n",
    "# 4 layer\n",
    "concat_4 = tf.concat((maxpool_3_bn_elu, x_4), axis = 3)\n",
    "weights_4 = init_weights([patch_dim[0], patch_dim[1], in_4, out_4], \n",
    "                    dist=dist,\n",
    "                    normalized=normalized_weights)\n",
    "conv_4 = conv2d(concat_4, weights_4)\n",
    "conv_4_elu = tf.nn.elu(conv_4)\n",
    "if bn == True:\n",
    "    conv_4_bn_elu = tf.contrib.layers.batch_norm(\n",
    "        conv_4_elu,\n",
    "        data_format='NHWC',\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        is_training=training,\n",
    "        decay=DECAY,\n",
    "        renorm=True)\n",
    "    maxpool_4_bn_elu = max_pool(conv_4_bn_elu, pool_dim)\n",
    "else:\n",
    "    maxpool_4_bn_elu = max_pool(conv_4_elu, pool_dim)\n",
    "\n",
    "weights[3] = weights_4  \n",
    "\n",
    "# 5 layer\n",
    "concat_5 = tf.concat((maxpool_4_bn_elu, x_5), axis = 3)\n",
    "weights_5 = init_weights([patch_dim5[0], patch_dim5[1], in_5, out_5], \n",
    "                    dist=dist,\n",
    "                    normalized=normalized_weights)\n",
    "conv_5 = conv2d(concat_5, weights_5)\n",
    "conv_5_elu = tf.nn.elu(conv_5)\n",
    "if bn == True:\n",
    "    conv_5_bn_elu = tf.contrib.layers.batch_norm(\n",
    "        conv_5_elu,\n",
    "        data_format='NHWC',\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        is_training=training,\n",
    "        decay=DECAY,\n",
    "        renorm=True)\n",
    "    maxpool_5_bn_elu = max_pool(conv_5_bn_elu, pool_dim)\n",
    "else:\n",
    "    maxpool_5_bn_elu = max_pool(conv_5_elu, pool_dim)\n",
    "\n",
    "weights[4] = weights_5 \n",
    "\n",
    "# _6 layer\n",
    "concat_6 = tf.concat((maxpool_5_bn_elu, x_6), axis = 3)\n",
    "weights_6 = init_weights([patch_dim6[0], patch_dim6[1], in_6, out_6], \n",
    "                    dist=dist,\n",
    "                    normalized=normalized_weights)\n",
    "conv_6 = conv2d(concat_6, weights_6)\n",
    "conv_6_elu = tf.nn.elu(conv_6)\n",
    "if bn == True:\n",
    "    conv_6_bn_elu = tf.contrib.layers.batch_norm(\n",
    "        conv_6_elu,\n",
    "        data_format='NHWC',\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        is_training=training,\n",
    "        decay=DECAY,\n",
    "        renorm=True)\n",
    "    maxpool_6_bn_elu = max_pool(conv_6_bn_elu, pool_dim)\n",
    "else:\n",
    "    maxpool_6_bn_elu = max_pool(conv_6_elu, pool_dim)\n",
    "\n",
    "weights[5] = weights_6 \n",
    "# FC\n",
    "\n",
    "fc1, weights[n_layers] = cnn.fully_connected(maxpool_6_bn_elu,\n",
    "                                        bn=True, \n",
    "                                        units=fc_units,\n",
    "                                        training=training,\n",
    "                                        nonlin=nonlin,\n",
    "                                        weights_dist=dist,\n",
    "                                        normalized_weights=normalized_weights)\n",
    "\n",
    "# Dropout (BN)\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# Readout\n",
    "weights[n_layers+1] = init_weights([fc_units, classes])\n",
    "y_conv = tf.matmul(fc1_drop, weights[n_layers+1])\n",
    "softmax_probs = tf.contrib.layers.softmax(y_conv)\n",
    "weights_shape = [tf.shape(el) for el in weights.values()]\n",
    "\n",
    "# LOSS\n",
    "loss = l2_loss(weights, \n",
    "            l2_regularization_penalty, \n",
    "            y_, \n",
    "            y_conv, \n",
    "            'loss')\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "prediction = tf.argmax(y_conv, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_test = hlp.subset_test(test_labels, classes)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "#n_test_list = []\n",
    "confusion_matrix_list = []\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "mean_per_class_accuracy_train_per_fold = []\n",
    "\n",
    "cross_validation_i = 0\n",
    "#for train_index, test_index in kf.split(data):  KFOLD\n",
    "for train_index, test_index in kf.split(data, np.argmax(targets[:,:], axis=1)):\n",
    "    cross_validation_i += 1\n",
    "    #print('####################################')\n",
    "    #print('         NUMERO   ', cross_validation_i, '/5')\n",
    "    #print('####################################')\n",
    "\n",
    "    ca_train = [ca[i][train_index, :, : , :] for i in range(len(ca))]\n",
    "    cd_train = [cd[i][train_index, :, : , :] for i in range(len(cd))]\n",
    "    train_labels = targets[train_index]\n",
    "\n",
    "    ca_test = [ca[i][test_index, :, : , :] for i in range(len(ca))]\n",
    "    cd_test = [cd[i][test_index, :, : , :] for i in range(len(cd))]\n",
    "    test_labels = targets[test_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #ind_test = hlp.subset_test(test_labels, classes)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Number of batches to train on\n",
    "        for i in range(n_iterations):\n",
    "            ind_train = hlp.subset_train(train_labels, classes, size_of_batches)\n",
    "\n",
    "            ## Every n iterations, print training accuracy\n",
    "            if i % 50 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                    x_1: cd_train[1][ind_train,:,:],\n",
    "                    x_2: cd_train[2][ind_train,:,:],\n",
    "                    x_3: cd_train[3][ind_train,:,:],\n",
    "                    x_4: cd_train[4][ind_train,:,:],\n",
    "                    x_5: cd_train[5][ind_train,:,:],\n",
    "                    x_6: cd_train[6][ind_train,:,:],\n",
    "                    y_: train_labels[ind_train,:],\n",
    "                    keep_prob: 1.0\n",
    "                    })\n",
    "                print('step %d, training accuracy: %g' % (\n",
    "                        i, train_accuracy))\n",
    "\n",
    "            # Training\n",
    "\n",
    "            train_step.run(feed_dict={\n",
    "                    x_1: cd_train[1][ind_train,:,:],\n",
    "                    x_2: cd_train[2][ind_train,:,:],\n",
    "                    x_3: cd_train[3][ind_train,:,:],\n",
    "                    x_4: cd_train[4][ind_train,:,:],\n",
    "                    x_5: cd_train[5][ind_train,:,:],\n",
    "                    x_6: cd_train[6][ind_train,:,:],\n",
    "                    y_: train_labels[ind_train,:],\n",
    "                    keep_prob: keep_prob_train\n",
    "                    })\n",
    "\n",
    "        ### TRAINING ACCURACY last step\n",
    "\n",
    "        y_pred = prediction.eval(feed_dict={\n",
    "                        x_1: cd_train[1][:,:,:],\n",
    "                        x_2: cd_train[2][:,:,:],\n",
    "                        x_3: cd_train[3][:,:,:],\n",
    "                        x_4: cd_train[4][:,:,:],\n",
    "                        x_5: cd_train[5][:,:,:],\n",
    "                        x_6: cd_train[6][:,:,:],\n",
    "                        y_: train_labels[:,:],\n",
    "                        keep_prob: 1.0\n",
    "                        })\n",
    "        # get mean of accuracy per class\n",
    "        acc_train, error_bar = cnn.mean_accuracy_per_class(np.argmax(train_labels[:,:],1), y_pred)\n",
    "        # append to list in order to save it\n",
    "        mean_per_class_accuracy_train_per_fold.append(acc_train)\n",
    "\n",
    "        ### TEST ACCURACY ###\n",
    "\n",
    "        # Print test accuracy on balanced test base\n",
    "        y_pred =  prediction.eval(feed_dict={\n",
    "            x_1: cd_test[1][:,:,:],\n",
    "            x_2: cd_test[2][:,:,:],\n",
    "            x_3: cd_test[3][:,:,:],\n",
    "            x_4: cd_test[4][:,:,:],\n",
    "            x_5: cd_test[5][:,:,:],\n",
    "            x_6: cd_test[6][:,:,:],\n",
    "            y_: test_labels[:,:],\n",
    "            keep_prob: 1.0\n",
    "            })\n",
    "        \n",
    "        acc, error_bar = cnn.mean_accuracy_per_class(np.argmax(test_labels[:,:],1), y_pred)\n",
    "        print('mean acc : %g +- %g ' % (acc, error_bar))         \n",
    "        \n",
    "\n",
    "        table_confusion = confusion_matrix(\n",
    "            np.argmax(test_labels[:,:],1),\n",
    "            y_pred)\n",
    "\n",
    "        # append\n",
    "        y_pred_list.append( list(y_pred) )\n",
    "        y_true_list.append( list( np.argmax(test_labels[:,:],1)) )\n",
    "        confusion_matrix_list.append(list(table_confusion)) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "cnf = np.array(confusion_matrix_list)\n",
    "mean_accuracy_per_class_per_fold = []\n",
    "error_bar_per_fold = []\n",
    "for k in range(cnf.shape[0]):\n",
    "    # Calculate average of accuracy of class on the k fold \n",
    "    n_test = np.sum(cnf[k], axis =1)\n",
    "    tp = np.diag(cnf[k])\n",
    "    fn = cnf[k].sum(axis=1) - np.diag(cnf[k])\n",
    "    \n",
    "    accuracy_per_class = tp/(tp+fn)\n",
    "    \n",
    "    mean_accuracy_per_class = np.mean( accuracy_per_class )\n",
    "    \n",
    "    error_bar = np.sqrt( np.sum( accuracy_per_class * (1 - accuracy_per_class)/n_test) ) /classes\n",
    "    \n",
    "    mean_accuracy_per_class_per_fold.append(mean_accuracy_per_class)\n",
    "    error_bar_per_fold.append(error_bar)\n",
    "    \n",
    "# result = mean on folds \n",
    "mean_accuracy_per_class = np.mean(mean_accuracy_per_class_per_fold)\n",
    "error_bar = np.sqrt(np.sum( np.array(error_bar_per_fold) **2 ))/len(error_bar_per_fold)\n",
    "\n",
    "    \n",
    "\n",
    "#acc_mean =  np.mean(acc_balanced_list)\n",
    "\n",
    "channels_in = [in_1, in_2, in_3, in_4, in_5, in_6]\n",
    "channels_out = [out_1, out_2, out_3, out_4, out_5, out_6]\n",
    "interval = align_on\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(n_splits):\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "    print('k-Fold num°', k+1)\n",
    "    \n",
    "    y_true = y_true_list[k]\n",
    "    y_pred = y_pred_list[k] \n",
    "       \n",
    "    \n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    recall_macro, error_bar, recall_macro_per_class, n_test = get_recall_macro(confusion_matrix)\n",
    "\n",
    "    classif_report_per_class = sklearn.metrics.classification_report(y_true, y_pred)\n",
    "    classif_report = rapport(confusion_matrix) \n",
    "\n",
    "\n",
    "    print('confusion matrix :')\n",
    "    print(confusion_matrix)\n",
    "    plt.figure()\n",
    "    classes_names = np.arange(1,classes+1,1)\n",
    "    plot_confusion_matrix(confusion_matrix, classes_names, normalize=True, title='Normalized Confusion matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print('\\nrecall_macro (moyenne des taux de réussite de chaque classe):')\n",
    "    print('average per-class effectiveness of a classifier to identify class labels')\n",
    "    print( round(100* recall_macro, 2), ' +- ', round(100*error_bar, 2), ' %')\n",
    "\n",
    "    print('\\nReport classifier :')\n",
    "    print(classif_report_per_class)\n",
    "\n",
    "    for key in classif_report:\n",
    "        print(key,' : ',classif_report[key] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mean on k_fold result\n",
    "recall_macro_per_kfold = np.zeros( (n_splits) )\n",
    "error_bar_per_kfold = np.zeros( (n_splits) )\n",
    "for k in range(n_splits):\n",
    "    \n",
    "    y_true = y_true_list[k]\n",
    "    y_pred = y_pred_list[k] \n",
    "    \n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    recall_macro, error_bar, recall_macro_per_class, n_test = get_recall_macro(confusion_matrix)\n",
    "    \n",
    "    recall_macro_per_kfold[k] = recall_macro # key start at 1\n",
    "    error_bar_per_kfold[k] = error_bar\n",
    "    \n",
    "print('recall macro sur chaque k_fold')\n",
    "print(recall_macro_per_kfold )\n",
    "    \n",
    "print('\\nMoyenne')\n",
    "recall_macro_mean = np.mean(recall_macro_per_kfold)\n",
    "\n",
    "print( round( 100 *recall_macro_mean, 1) )\n",
    "\n",
    "print(\"\\nBarre d'erreur\")\n",
    "error_bar = np.sqrt( np.sum( error_bar_per_kfold**2) )/classes\n",
    "\n",
    "\n",
    "print( round(100 *error_bar, 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(recall_macro_per_kfold, ddof=1 )*100/np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_all = np.sum(cnf, axis = 0)\n",
    "recall_macro, error_bar, recall_macro_per_class, n_test = get_recall_macro(confusion_matrix_all)\n",
    "\n",
    "print('\\nMoyenne')\n",
    "print(round(100*recall_macro,1))\n",
    "print('\\nError bar')\n",
    "print(round(100 *error_bar, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
